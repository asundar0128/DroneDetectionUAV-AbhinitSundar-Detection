{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGwBNlMJQmjOtggVaS8Rjq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzgmyBtuu0tb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import SequentialSampler\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHCzeEFj81Qf",
        "outputId": "16c70bd2-e810-4a84-a757-03d6fa0308d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import matplotlib.patches as patches\n",
        "from utils import *\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import ops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "UDR3Chpx8TEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6mdumcqu_LW",
        "outputId": "e037d877-5706-4672-b92c-42754dc4def2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "firstDroneVideoPath = \"/content/drive/MyDrive/DroneDetection/Drone Tracking 1.mp4\"\n",
        "secondDroneVideoPath = \"/content/drive/MyDrive/DroneDetection/Drone Tracking 2.mp4\"\n",
        "primaryDatasetPath = \"/content/drive/MyDrive/DroneDetection/faster-rcnn-drone-detection-pytorch.ipynb\""
      ],
      "metadata": {
        "id": "UbvUHQ3Uvsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DroneDetection(Dataset):\n",
        "    def __init__(self, primaryDatasetPath):\n",
        "        self.primaryDatasetPath = primaryDatasetPath\n",
        "\n",
        "        self.entireDroneImageDataset, self.anchorBoxesDroneDataset, self.droneImagesClasses = self.get_data()\n",
        "\n",
        "    def droneDatasetLength(self):\n",
        "        return self.entireDroneImageDataset.size(dim=0)\n",
        "\n",
        "    def droneDatasetRetrieveItem(self, droneDatasetindex):\n",
        "        return self.entireDroneImageDataset[droneDatasetindex], self.anchorBoxesDroneDataset[droneDatasetindex], self.droneImagesClasses[droneDatasetindex]\n",
        "\n",
        "    def droneDatasetRetrieveData(self):\n",
        "        entireDroneImageDataset = []\n",
        "        droneDatasetRetrieveIndices = []\n",
        "\n",
        "        boxesDroneDataset, droneImagesClasses, droneDatasetImagePaths = droneDatasetJoinAnnotations(self.droneDetectionAnnotationPath, self.droneDetectionImageDirectory, self.droneDetectionImageSize)\n",
        "\n",
        "        for m, droneDatasetImagePath in enumerate(droneDatasetImagePaths):\n",
        "            #the object detection dataset skips past the drone's image classes and path if the path specified is not valid\n",
        "            if (not droneDatasetImagePath) or (not os.path.exists(droneDatasetImagePath)):\n",
        "                continue\n",
        "            # the image is read and resized using the io library and the imread/resize methods.\n",
        "            droneDatasetReadingImage = io.imread(droneDatasetImagePath)\n",
        "            droneDatasetReadingImage = resize(droneDatasetReadingImage, self.droneDetectionImageSize)\n",
        "\n",
        "            #The image is subsequently converted to torch tensor and reshaped so that channels come first\n",
        "\n",
        "            droneDatasetImageTensor = torch.from_numpy(droneDatasetReadingImage).permute(2, 0, 1)\n",
        "\n",
        "            #the specific drone class names are encoded as integers\n",
        "            droneClasses = droneImagesClasses[m]\n",
        "            droneDatasetIndices = torch.Tensor([self.droneDetectionIndexValue[droneDatasetName] for droneDatasetName in droneClasses])\n",
        "\n",
        "            entireDroneImageDataset.append(droneDatasetImageTensor)\n",
        "            droneDatasetRetrieveIndices.append(droneDatasetIndices)\n",
        "\n",
        "            # the bounding boxes and classes are padded so that they are of equal size\n",
        "\n",
        "        droneDatasetRetrievePaddedBoundingBoxes = droneDatasetPaddingValue(boxesDroneDataset, batch_first=True, padding_value=-1)\n",
        "        droneDatasetPaddedClasses = droneDatasetPaddingValue(droneDatasetRetrieveIndices, batch_first=True, padding_value=-1)\n",
        "\n",
        "            #all images across the entire dataset are stacked using the torch module\n",
        "        droneDatasetStackedImage = torch.stack(entireDroneImageDataset, dim=0)\n",
        "\n",
        "            #the stacked image for the drone dataset is returned and the retrieved padded bounding boxes and padded classes are sent to\n",
        "            #the console for output to user as well\n",
        "        return droneDatasetStackedImage.to(dtype=torch.float32), droneDatasetRetrievePaddedBoundingBoxes, droneDatasetPaddedClasses"
      ],
      "metadata": {
        "id": "TEeQdYhqwa-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The drone dataset model is fed into resnet50 a Fast-RCNN model using torchvision\n",
        "droneDatasetModel = torchvision.models.resnet50(pretrained = True)\n",
        "\n",
        "\n",
        "droneDatasetRequiredLayers = list(droneDatasetModel.children())[:8]\n",
        "droneDatasetBackbone = nn.Sequential(*droneDatasetRequiredLayers)\n",
        "\n",
        "#All the parameters for droneDataset are unfrozen\n",
        "\n",
        "for droneDatasetParameter in droneDatasetBackbone.named_parameters():\n",
        "  droneDatasetParameter[1].requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYziQ9kH-EOE",
        "outputId": "d3f3b1db-3462-486a-f6c6-6342f4c53c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drone dataset anchor is generated using the function below with the output size passed as parameter to it\n",
        "def droneDetectionGeneratingAnchorCenter(droneDetectionOutputSize):\n",
        "  #The output height and output width for the drone dataset are equivalent to the drone detection output size\n",
        "  droneDatasetOutputHeight, droneDatasetOutputWeight = droneDetectionOutputSize\n",
        "\n",
        "#The drone detection X Anchor Points and the drone detection Y anchor points are assigned to the Drone Dataset Output Width and Height,\n",
        "#with 0.5 added for bias\n",
        "  droneDetectionXAnchorPoints = torch.arange(0, droneDatasetOutputWeight) + 0.5\n",
        "  droneDetectionYAnchorPoints = torch.arange(0, droneDatasetOutputHeight) + 0.5\n",
        "#The X and Y anchor points are returned as output for the drone Detection anchor bounding box protocol.\n",
        "  return droneDetectionXAnchorPoints, droneDetectionYAnchorPoints"
      ],
      "metadata": {
        "id": "xwycb2l4iqsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The anchor box for the drone detection is generated with this function below with the X and Y anchor points as\n",
        "#coordinates and the scale, ratio, and output size as additional parameters for this function\n",
        "def droneDetectionGeneratingAnchorBox(droneDetectionXAnchorPoints, droneDetectionYAnchorPoints, droneDetectionAnchorScale, droneDetectionAnchorRatio, droneDetectionOutputSize):\n",
        "  #the number of drone detection anchor bozes is equal to the length of the drone detection anchor scale multiplied\n",
        "  #by the length of the drone detection anchor ratio\n",
        "\n",
        "    droneDetectionNumberAnchorBoxes = len(droneDetectionAnchorScale) * len(droneDetectionAnchorRatio)\n",
        "    droneDetectionAnchorBase = torch.zeros(1, droneDetectionXAnchorPoints.size(dim=0) \\\n",
        "                              , droneDetectionYAnchorPoints.size(dim=0), droneDetectionNumberAnchorBoxes, 4)\n",
        "\n",
        "    for rowIndexXAnchorPoint, columnIndexXAnchorPoint in enumerate(droneDetectionXAnchorPoints):\n",
        "        for rowIndexYAnchorPoint, columnIndexYAnchorPoint in enumerate(droneDetectionYAnchorPoints):\n",
        "            droneDetectionAnchorBoxValue = torch.zeros((droneDetectionNumberAnchorBoxes, 4))\n",
        "            q = 0\n",
        "            for w, droneDetectionScale in enumerate(droneDetectionAnchorScale):\n",
        "                for e, droneDetectionRatio in enumerate(droneDetectionAnchorRatio):\n",
        "                    p = droneDetectionScale * droneDetectionRatio\n",
        "                    d = droneDetectionScale\n",
        "\n",
        "                    droneDetectionMinimumXValue = columnIndexXAnchorPoint - p / 2\n",
        "                    droneDetectionMinimumYValue = columnIndexYAnchorPoint - d / 2\n",
        "                    droneDetectionMaximumXValue = columnIndexXAnchorPoint + p / 2\n",
        "                    droneDetectionMaximumYValue = columnIndexYAnchorPoint + d / 2\n",
        "\n",
        "                    droneDetectionAnchorBoxValue[c, :] = torch.Tensor([droneDetectionMinimumXValue, droneDetectionMinimumYValue, droneDetectionMaximumXValue, droneDetectionMaximumYValue])\n",
        "                    c += 1\n",
        "\n",
        "            droneDetectionAnchorBase[:, rowIndexXAnchorPoint, rowIndexYAnchorPoint, :] = ops.clip_boxes_to_image(droneDetectionAnchorBoxValue, size=droneDetectionOutputSize)\n",
        "\n",
        "    return droneDetectionAnchorBase"
      ],
      "metadata": {
        "id": "G88PPW16LN3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The drone dataset IOU matrix is retrieved with its batch size, total anchor boxes, and retrieved padded bounding boxes\n",
        "def droneDatasetRetrieveIOUMatrix(droneDatasetBatchSize, droneDatasetEntireAnchorBoxes, droneDatasetRetrievePaddedBoundingBoxes):\n",
        "#Flattening the entire anchor boxes and reshaping it with the batch size of the drone dataset will yield the flattened anchor boxes\n",
        "#for the drone dataset\n",
        "    droneDatasetFlattenAnchorBoxes = droneDatasetEntireAnchorBoxes.reshape(droneDatasetBatchSize, -1, 4)\n",
        "#The complete anchor boxes will be the flattened anchor boxes for the drone\n",
        "    droneDatasetTotalAnchorBoxes = droneDatasetFlattenAnchorBoxes.size(dim=1)\n",
        "#The IOU matrix for the drone dataset will be invoked using the torch library with the batch size,\n",
        "#total anchor boxes, and the retrieved padded bounding boxes\n",
        "    droneDatasetIOUMatrix = torch.zeros((droneDatasetBatchSize, droneDatasetTotalAnchorBoxes, droneDatasetRetrievePaddedBoundingBoxes.size(dim=1)))\n",
        "#Looping throuh the batch size for the drone dataset will create a bounding box value from the retrieve bounding box function\n",
        "#and a flattened anchor boxes for w iterations and a IOU matrix with the anchor boxes and drone dataset\n",
        "    for w in range(droneDatasetBatchSize):\n",
        "        droneDatasetBoundingBoxValue = droneDatasetRetrievePaddedBoundingBoxes[w]\n",
        "        droneDatasetAnchorBoxes = droneDatasetFlattenAnchorBoxes[w]\n",
        "        droneDatasetIOUMatrix[w, :] = droneDatasetOPS.droneDatasetIOUBoxes(droneDatasetAnchorBoxes, droneDatasetBoundingBoxValue)\n",
        "\n",
        "    return droneDatasetIOUMatrix"
      ],
      "metadata": {
        "id": "VyrJN-EziKaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def droneDatasetProjectBoundingBoxes(droneDatasetBoundingBoxes, droneDatasetWidthFactor, droneDatasetHeightFactor, droneDatasetMode='a2p'):\n",
        "    #The two modes specified here are activation mode to pixel image and pixel mode to activation image\n",
        "    assert droneDatasetMode in ['a2p', 'p2a']\n",
        "\n",
        "#The drone dataset batch size is equal to its bounding boxes with a size of its dataset's collection\n",
        "#of images\n",
        "    droneDatasetBatchSize = droneDatasetBoundingBoxes.size(dim=0)\n",
        "#The projected bounding boxes will be equal to the bounding boxes of the current drone dataset reshaped\n",
        "    droneDatasetProjectedBoundingBoxes = droneDatasetBoundingBoxes.clone().reshape(droneDatasetBatchSize, -1, 4)\n",
        "    droneDatasetInvalidBoundingBox = (droneDatasetProjectedBoundingBoxes == -1)\n",
        "#If the drone dataset is of activation to pixel mode, multiply the bounding box indices by the width\n",
        "#or height ratio\n",
        "    if droneDatasetMode == 'a2p':\n",
        "        droneDatasetProjectedBoundingBoxes[:, :, [0, 2]] *= droneDatasetWidthFactor\n",
        "        droneDatasetProjectedBoundingBoxes[:, :, [1, 3]] *= droneDatasetHeightFactor\n",
        "#If the drone dataset is of pixel to activation mode, divide the bounding box indices by the\n",
        "#width or height ratio\n",
        "    else:\n",
        "        droneDatasetProjectedBoundingBoxes[:, :, [0, 2]] /= droneDatasetWidthFactor\n",
        "        droneDatasetProjectedBoundingBoxes[:, :, [1, 3]] /= droneDatasetHeightFactor\n",
        "\n",
        "    droneDatasetProjectedBoundingBoxes.masked_fill_(droneDatasetInvalidBoundingBox, -1)\n",
        "    droneDatasetProjectedBoundingBoxes.resize_as_(droneDatasetBoundingBoxes)\n",
        "\n",
        "    return droneDatasetProjectedBoundingBoxes"
      ],
      "metadata": {
        "id": "NfsIwCqBWNOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the drone dataset offset value by taking in the positive anchor coordinates and the\n",
        "#bounding box mapping\n",
        "\n",
        "def droneDatasetCalculateOffset(droneDatasetPositiveAnchorCoordinates, droneDatasetBoundingBoxMapping):\n",
        "  #The drone dataset positive anchor coordinates is equal the converted coordinates\n",
        "  #with the input and output format\n",
        "    droneDatasetPositiveAnchorCoordinates = ops.box_convert(droneDatasetPositiveAnchorCoordinates, droneDatasetInputFormat='ewrt', droneDatasetOutputFormat='dfgsaa')\n",
        "    droneDatasetBoundingBoxMapping = ops.box_convert(droneDatasetBoundingBoxMapping, droneDatasetInputFormat='ewrt', droneDatasetOutputFormat='dfgsaa')\n",
        "\n",
        "    droneDatasetBoundingBoxXCoordinate, droneDatasetBoundingBoxYCoordinate, droneDatasetBoundingBoxWidth, droneDatasetBoundingBoxHeight = droneDatasetBoundingBoxMapping[:, 0], droneDatasetBoundingBoxMapping[:, 1], droneDatasetBoundingBoxMapping[:, 2], droneDatasetBoundingBoxMapping[:, 3]\n",
        "    droneDatasetAnchorXCoordinate, droneDatasetAnchorYCoordinate, droneDatasetAnchorWidth, droneDatasetAnchorHeight = droneDatasetPositiveAnchorCoordinates[:, 0], droneDatasetPositiveAnchorCoordinates[:, 1], droneDatasetPositiveAnchorCoordinates[:, 2], droneDatasetPositiveAnchorCoordinates[:, 3]\n",
        "\n",
        "    droneDatasetTorchXCoordinate_ = (droneDatasetBoundingBoxXCoordinate - droneDatasetAnchorXCoordinate)/droneDatasetAnchorWidth\n",
        "    droneDatasetTorchYCoordinate_ = (droneDatasetBoundingBoxYCoordinate - droneDatasetAnchorYCoordinate)/droneDatasetAnchorHeight\n",
        "    droneDatasetTorchWidth_ = torch.log(droneDatasetBoundingBoxWidth / droneDatasetAnchorWidth)\n",
        "    droneDatasetTorchHeight = torch.log(droneDatasetBoundingBoxHeight / droneDatasetAnchorHeight)\n",
        "\n",
        "    return torch.stack([droneDatasetTorchXCoordinate_, droneDatasetTorchYCoordinate_, droneDatasetTorchWidth_, droneDatasetTorchHeight], dim=-1)"
      ],
      "metadata": {
        "id": "bjOqJZFzUVw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function retrieves the required anchors for the drone dataset\n",
        "\n",
        "def droneDatasetRequiredAnchors(droneDatasetEntireAnchorBoxes, droneDatasetObtainBoundingBoxes, droneDatasetClasses, droneDatasetPositiveIOUThreshold=0.6, droneDatasetNegativeIOUThreshold=0.3):\n",
        "\n",
        "#For the drone dataset, retrieve the size and shape parameter values with P and Y as the variables\n",
        "    P, droneDetectionWidthAnchorMap, droneDetectionHeightAnchorMap, O, _ = droneDatasetEntireAnchorBoxes.shape\n",
        "    Y= droneDatasetObtainBoundingBoxes.shape[1]\n",
        "#The total number of anchor boxes in a single image is obtained\n",
        "    droneDatasetTotalAnchorBoxes = O * droneDetectionWidthAnchorMap * droneDetectionHeightAnchorMap\n",
        "#The IOU Matrix is obtained which has the IOU of every anchor box, in comparison\n",
        "#with the collection of ground truth bounding boxes in an image\n",
        "    droneDatasetIOUMatrix = droneDatasetRetrieveIOUMatrix(P, droneDatasetEntireAnchorBoxes, droneDatasetObtainBoundingBoxes)\n",
        "\n",
        "    droneDatasetMaximumValuePerBoundingBox, _ = droneDatasetIOUMatrix.max(dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "    droneDatasetPositiveAnchorMask = torch.logical_and(droneDatasetIOUMatrix == droneDatasetMaximumValuePerBoundingBox, droneDatasetMaximumValuePerBoundingBox > 0)\n",
        "    droneDatasetPositiveAnchorMask = torch.logical_or(droneDatasetPositiveAnchorMask, droneDatasetIOUMatrix > droneDatasetPositiveIOUThreshold)\n",
        "\n",
        "    droneDatasetPositiveAnchorIndexSeparation = torch.where(droneDatasetPositiveAnchorMask)[0] # get separate indices in the batch\n",
        "\n",
        "    droneDatasetPositiveAnchorMask = droneDatasetPositiveAnchorMask.flatten(start_dim=0, end_dim=1)\n",
        "    droneDatasetPositiveAnchorIndex = torch.where(droneDatasetPositiveAnchorMask)[0]\n",
        "\n",
        "\n",
        "\n",
        "    droneDatasetMaximumIOUPerAnchor, droneDatasetMaximumIOUPerAnchorIndex = droneDatasetIOUMatrix.max(dim=-1)\n",
        "    droneDatasetMaximumIOUPerAnchor = droneDatasetMaximumIOUPerAnchor.flatten(start_dim=0, end_dim=1)\n",
        "\n",
        "\n",
        "    droneDatasetConfidenceScores = droneDatasetMaximumIOUPerAnchor[droneDatasetPositiveAnchorIndex]\n",
        "\n",
        "    droneDatasetBoundingClassesExpanded = droneDatasetClasses.view(P, 1, Y).expand(P, droneDatasetTotalAnchorBoxes, Y)\n",
        "    droneDatasetBoundingClasses = torch.gather(droneDatasetBoundingClassesExpanded, -1, droneDatasetMaximumIOUPerAnchorIndex.unsqueeze(-1)).squeeze(-1)\n",
        "    droneDatasetBoundingClasses = droneDatasetBoundingClasses.flatten(start_dim=0, end_dim=1)\n",
        "    droneDatasetBoundingClassesPositive = droneDatasetBoundingClasses[droneDatasetPositiveAnchorIndex]\n",
        "\n",
        "\n",
        "    droneDatasetBoundingBoxesExpanded = droneDatasetObtainBoundingBoxes.view(P, 1, Y, 4).expand(P, droneDatasetTotalAnchorBoxes, Y, 4)\n",
        "\n",
        "    droneDatasetBoundingBoxes = torch.gather(droneDatasetBoundingClassesExpanded, -2, droneDatasetMaximumIOUPerAnchorIndex.reshape(P, droneDatasetTotalAnchorBoxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "\n",
        "    droneDatasetBoundingBoxes = droneDatasetBoundingBoxes.flatten(start_dim=0, end_dim=2)\n",
        "    droneDatasetBoundingBoxesPositive = droneDatasetBoundingBoxes[droneDatasetPositiveAnchorIndex]\n",
        "\n",
        "    droneDatasetAnchorBoxesFlattened = droneDatasetEntireAnchorBoxes.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    droneDatasetPositiveAnchorCoordinates = droneDatasetAnchorBoxesFlattened[droneDatasetPositiveAnchorIndex]\n",
        "\n",
        "    droneDatasetOffsetValue = droneDatasetCalculateOffset(droneDatasetPositiveAnchorCoordinates, droneDatasetBoundingBoxesPositive)\n",
        "\n",
        "\n",
        "    droneDatasetNegativeAnchorMask = (droneDatasetMaximumIOUPerAnchor < droneDatasetNegativeIOUThreshold)\n",
        "    droneDatasetNegativeAnchorIndex = torch.where(droneDatasetNegativeAnchorMask)[0]\n",
        "\n",
        "    droneDatasetNegativeAnchorIndex = droneDatasetNegativeAnchorIndex[torch.randint(0, droneDatasetNegativeAnchorIndex.shape[0], (droneDatasetPositiveAnchorIndex.shape[0],))]\n",
        "    droneDatasetNegativeAnchorCoordinates = droneDatasetAnchorBoxesFlattened[droneDatasetNegativeAnchorIndex]\n",
        "\n",
        "    return droneDatasetPositiveAnchorIndex, droneDatasetNegativeAnchorIndex, droneDatasetConfidenceScores, droneDatasetOffsetValue, droneDatasetBoundingClassesPositive, \\\n",
        "         droneDatasetPositiveAnchorCoordinates, droneDatasetNegativeAnchorCoordinates, droneDatasetPositiveAnchorIndexSeparation"
      ],
      "metadata": {
        "id": "WFkRnTYQPIWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class droneDatasetProposalModule(nn.Module):\n",
        "    def __init__(self, droneDatasetInputFeatures, droneDatasetHiddenDimensions=512, droneDatasetNumberAnchors=9, droneDatasetProbabilityDropout=0.3):\n",
        "        super().__init__()\n",
        "        self.droneDatasetNumberAnchors = droneDatasetNumberAnchors\n",
        "        self.droneDatasetConvolutionalLayer = nn.Conv2d(droneDatasetInputFeatures, droneDatasetHiddenDimensions, droneDatasetKernelSize=3, droneDatasetPadding=1)\n",
        "        self.droneDatasetDropoutValue = nn.Dropout(droneDatasetProbabilityDropout)\n",
        "        self.droneDatasetConfidenceHead = nn.Conv2d(droneDatasetHiddenDimensions, droneDatasetNumberAnchors, droneDatasetKernelSize=1)\n",
        "        self.droneDatasetRegularHead = nn.Conv2d(droneDatasetHiddenDimensions, droneDatasetNumberAnchors * 4, droneDatasetKernelSize=1)\n",
        "\n",
        "    def forward(self, feature_map, droneDatasetPositiveAnchorIndex=None, droneDatasetNegativeAnchorIndex=None, droneDatasetPositiveAnchorCoordinates=None):\n",
        "\n",
        "        if droneDatasetPositiveAnchorIndex is None or droneDatasetNegativeAnchorIndex is None or droneDatasetPositiveAnchorCoordinates is None:\n",
        "            droneDatasetOption = 'eval'\n",
        "        else:\n",
        "            droneDatasetOption = 'train'\n",
        "\n",
        "        droneDatasetOutput = self.droneDatasetConvolutionalLayer(feature_map)\n",
        "        droneDatasetOutput = F.relu(self.droneDatasetDropoutValue(droneDatasetOutput))\n",
        "\n",
        "        droneDatasetRegularOffsetPrediction = self.droneDatasetRegularHead(droneDatasetOutput)\n",
        "        droneDatasetConfidenceScoresPrediction = self.droneDatasetConfidenceHead(droneDatasetOutput)\n",
        "#retrieving the confidence scores provided the drone dataset option is equivalent to train\n",
        "        if droneDatasetOption == 'train':\n",
        "\n",
        "            droneDatasetConfidenceScorePositive = droneDatasetConfidenceScoresPrediction.flatten()[droneDatasetPositiveAnchorIndex]\n",
        "            droneDatasetConfidenceScoreNegative = droneDatasetConfidenceScoresPrediction.flatten()[droneDatasetNegativeAnchorIndex]\n",
        "#the offset values for positive anchors is obtained\n",
        "            droneDatasetOffsetPosition = droneDatasetRegularOffsetPrediction.contiguous().view(-1, 4)[droneDatasetPositiveAnchorIndex]\n",
        "#generated proposals utilizing the offset values in previous step is obtained\n",
        "            droneDatasetProposal = droneDatasetProposalModule(droneDatasetPositiveAnchorCoordinates, droneDatasetOffsetPosition)\n",
        "\n",
        "            return droneDatasetConfidenceScorePositive, droneDatasetConfidenceScoreNegative, droneDatasetOffsetPosition, droneDatasetProposal\n",
        "#if the drone dataset option is evaluation, return the drone dataset confidence scores prediction\n",
        "#and regular offset prediction\n",
        "        elif droneDatasetOption == 'eval':\n",
        "            return droneDatasetConfidenceScoresPrediction, droneDatasetRegularOffsetPrediction"
      ],
      "metadata": {
        "id": "eLTQWI9kdFPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def droneDatasetProposalCreation(droneDatasetAnchorValue, droneDatasetOffsetValue):\n",
        "\n",
        "    droneDatasetAnchors = ops.box_convert(droneDatasetAnchors, droneDatasetInputFormat='ewrt', droneDatasetOutputFormat='cxcywh')\n",
        "\n",
        "    droneDatasetProposalValue = torch.zeros_like(droneDatasetAnchors)\n",
        "    droneDatasetProposalValue[:,0] = droneDatasetAnchors[:,0] + droneDatasetOffsetValue[:,0]*droneDatasetAnchors[:,2]\n",
        "    droneDatasetProposalValue[:,1] = droneDatasetAnchors[:,1] + droneDatasetOffsetValue[:,1]*droneDatasetAnchors[:,3]\n",
        "    droneDatasetProposalValue[:,2] = droneDatasetAnchors[:,2] * torch.exp(droneDatasetOffsetValue[:,2])\n",
        "    droneDatasetProposalValue[:,3] = droneDatasetAnchors[:,3] * torch.exp(droneDatasetOffsetValue[:,3])\n",
        "\n",
        "    droneDatasetProposalValue = ops.box_convert(droneDatasetProposalValue, droneDatasetInputFormat='ewrt', droneDatasetOutputFormat='cxcywh')\n",
        "\n",
        "    return droneDatasetProposalValue"
      ],
      "metadata": {
        "id": "yrUm7zzJhUhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageDetector(nn.Module):\n",
        "    def __init__(self, droneDatasetImageSize, droneDatasetOutputSize, droneDatasetOutputChannels, droneDatasetNumberClasses, droneDatasetROISize):\n",
        "        super().__init__()\n",
        "        self.regionProposalNetwork = droneDatasetRegionProposalNetwork(droneDatasetImageSize, droneDatasetOutputSize, droneDatasetOutputChannels)\n",
        "        self.droneDatasetClassifier = ClassificationModule(droneDatasetOutputChannels, droneDatasetNumberClasses, droneDatasetROISize)\n",
        "\n",
        "    def droneDatasetForward(self, droneDatasetImages, droneDatasetBoundingBoxes, droneDatasetClasses):\n",
        "        droneDatasetRPNCumulativeLoss, droneDatasetFeatureMap, droneDatasetProposalValue, \\\n",
        "        droneDatasetPositiveAnchorIndexSeparation, droneDatasetClassPositive = self.droneDatasetRPNValue(droneDatasetImageValue, droneDatasetBoundingBox, droneDatasetClasses)\n",
        "\n",
        "        droneDatasetPositiveProposalList = []\n",
        "        droneDatasetBatchSize = droneDatasetImages.size(dim=0)\n",
        "        for droneDatasetIndex in range(droneDatasetBatchSize):\n",
        "            droneDatasetProposalIndices = torch.where(droneDatasetPositiveAnchorIndexSeparation == droneDetectionIndex)[0]\n",
        "            droneDatasetProposalSeparation = droneDatasetProposalCreation[droneDatasetProposalIndex].detach().clone()\n",
        "            droneDatasetPositiveProposalList.append(droneDatasetProposalSeparation)\n",
        "\n",
        "        droneDatasetConfidenceLoss = self.droneDatasetClassifier(droneDatasetFeatureMap, droneDatasetPositiveProposalList, droneDatasetClassPositive)\n",
        "        droneDatasetTotalLoss = droneDatasetConfidenceLoss + droneDatasetRPNLoss\n",
        "\n",
        "        return droneDatasetTotalLoss\n",
        "\n",
        "    def droneDatasetInferenceValue(self, droneDatasetImages, droneDatasetConfidenceValue=0.5, droneDatasetThreshold=0.7):\n",
        "        droneDetectionBatchSize = droneDatasetImages.size(dim=0)\n",
        "        droneDetectionFinalProposal, droneDetectionConfidenceScoreFinal, droneDetectionFeatureMap = self.droneDatasetRegionProposalNetwork.droneDatasetInferenceValue(droneDetectionImages, droneDatasetConfidenceValue, droneDatasetThreshold)\n",
        "        droneDetectionConfidenceScore = self.classifier(droneDetectionFeatureMap, droneDetectionFinalProposal)\n",
        "\n",
        "        droneDetectionConfidenceProbability = F.softmax(droneDetectionConfidenceScore, dim=-1)\n",
        "        droneDetectionTotalClasses = torch.argmax(droneDetectionConfidenceProbability, dim=-1)\n",
        "\n",
        "        droneDetectionFinalClasses = []\n",
        "        w = 0\n",
        "        for u in range(droneDetectionBatchSize):\n",
        "            droneDetectionNumberProposals = len(droneDetectionFinalProposal[u]) # get the number of proposals for each image\n",
        "            droneDetectionFinalClasses.append(droneDetectionTotalClasses[w: w+droneDetectionNumberProposals])\n",
        "            w += droneDetectionNumberProposals\n",
        "\n",
        "        return droneDetectionFinalProposal, droneDetectionConfidenceScore, droneDetectionFinalClasses"
      ],
      "metadata": {
        "id": "4doi3fdWgxJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def droneDetectionTrainingFastRCNN(droneDatasetModelValue, droneDatasetLearningRate, droneDatasetTrainingDataLoader, droneDatasetNumberEpochs):\n",
        "\n",
        "  droneDatasetOptimizer = torch.optim.Adam(droneDatasetModelValue.parameters(), droneDatasetLR = droneDatasetLearningRate)\n",
        "\n",
        "  droneDatasetModelValue.train()\n",
        "  droneDatasetLossListCollection = []\n",
        "\n",
        "  for v in droneDatasetLossFunction(range(droneDatasetNumberEpochs)):\n",
        "    droneDatasetTotalLoss = 0\n",
        "    for droneDatasetImageBatch, droneDatasetBoundingBoxesBatch, droneDatasetClassesBatch in droneDatasetTrainingDataLoader:\n",
        "\n",
        "      droneDatasetLossValue = droneDatasetModelValue(droneDatasetImageBatch, droneDatasetBoundingBoxesBatch, droneDatasetClassesBatch)\n",
        "\n",
        "      droneDatasetOptimizer.zero_grad()\n",
        "      droneDatasetLossValue.backward()\n",
        "      droneDatasetOptimizer.step()\n",
        "\n",
        "      droneDatasetTotalLoss += droneDatasetLossValue.item()\n",
        "\n",
        "    droneDatasetLossListCollection.append(droneDatasetTotalLoss)\n",
        "\n",
        "  return droneDatasetLossListCollection"
      ],
      "metadata": {
        "id": "yTIMVJIphbLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "\n",
        "def droneDatasetFrameCapture(droneFirstVideoPath):\n",
        "\n",
        "    droneFirstVideoPath = \"/content/drive/MyDrive/DroneDetection/Drone Tracking 1.mp4\"\n",
        "\n",
        "    droneVideoObject = cv2.VideoCapture(droneFirstVideoPath)\n",
        "\n",
        "    droneVideoCount = 0\n",
        "\n",
        "    droneVideoSuccess = 1\n",
        "\n",
        "    while droneVideoSuccess:\n",
        "\n",
        "        droneVideoSuccess, droneVideoImageValue = droneVideoObject.read()\n",
        "\n",
        "        #cv2.imwrite(\"frame%d.mp4\" % droneVideoCount, droneFirstVideoPath)\n",
        "\n",
        "        droneVideoCount += 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    droneDatasetFrameCapture(\"C:\\\\Users\\\\sheel\\\\OneDrive\\\\Desktop\\\\Drone Tracking 1.mp4\")"
      ],
      "metadata": {
        "id": "VPSKoTrjhhPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def droneDatasetFrameCapture(droneFirstVideoPath):\n",
        "\n",
        "    droneSecondVideoPath = \"/content/drive/MyDrive/DroneDetection/Drone Tracking 2.mp4\"\n",
        "\n",
        "    droneVideoObject = cv2.VideoCapture(droneSecondVideoPath)\n",
        "\n",
        "    droneVideoCount = 0\n",
        "\n",
        "    droneVideoSuccess = 1\n",
        "\n",
        "    while droneVideoSuccess:\n",
        "\n",
        "        droneVideoSuccess, droneVideoImageValue = droneVideoObject.read()\n",
        "\n",
        "        #cv2.imwrite(\"frame%d.mp4\" % droneVideoCount, droneSecondVideoPath)\n",
        "\n",
        "\n",
        "        droneVideoCount += 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    droneDatasetFrameCapture(\"C:\\\\Users\\\\sheel\\\\OneDrive\\\\Desktop\\\\Drone Tracking 2.mp4\")"
      ],
      "metadata": {
        "id": "4RFSWFHGqc1-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}